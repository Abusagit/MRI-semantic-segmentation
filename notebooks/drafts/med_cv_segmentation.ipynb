{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Sequence\n",
    "from torchvision.transforms import functional as F\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchmetrics as TM\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Convert a pytorch tensor into a PIL image\n",
    "t2img = T.ToPILImage()\n",
    "# Convert a PIL image into a pytorch tensor\n",
    "img2t = T.ToTensor()\n",
    "\n",
    "# Set the working (writable) directory.\n",
    "working_dir = Path(\"/home/fvelikon/projects/mri_cv/data/processed_data_split\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "def save_model_checkpoint(model, cp_name):\n",
    "    torch.save(model.state_dict(), os.path.join(working_dir, cp_name))\n",
    " \n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Load model from saved checkpoint\n",
    "def load_model_from_checkpoint(model, ckp_path):\n",
    "    return model.load_state_dict(\n",
    "        torch.load(\n",
    "            ckp_path,\n",
    "            map_location=get_device(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Send the Tensor or Model (input argument x) to the right device\n",
    "# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n",
    "# otherwise send to CPU.\n",
    "def to_device(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x.cpu()\n",
    "    \n",
    "def get_model_parameters(m):\n",
    "    total_params = sum(\n",
    "        param.numel() for param in m.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "def print_model_parameters(m):\n",
    "    num_model_parameters = get_model_parameters(m)\n",
    "    print(f\"The Model has {num_model_parameters/1e6:.2f}M parameters\")\n",
    "# end if\n",
    "\n",
    "def close_figures():\n",
    "    while len(plt.get_fignums()) > 0:\n",
    "        plt.close()\n",
    "    # end while\n",
    "# end def\n",
    "\n",
    "# Validation: Check if CUDA is available\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root: Path, split_mode: str, \n",
    "                 transform: Optional[T.Compose]=None, \n",
    "                 target_transform: Optional[T.Compose]=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.split_dir = root / split_mode\n",
    "        \n",
    "        self._filenames = list(sorted([path.name for path in self.split_dir.glob(\"*.pt\")]))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        filename = self._filenames[index]\n",
    "        \n",
    "        image_and_mask_tensor: torch.Tensor = torch.load(self.split_dir / filename)\n",
    "        \n",
    "        image, mask = image_and_mask_tensor\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MRIDataset(root=working_dir, split_mode=\"train\")\n",
    "test_ds = MRIDataset(root=working_dir, split_mode=\"test\")\n",
    "eval_ds = MRIDataset(root=working_dir, split_mode=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([397, 772]), torch.Size([397, 772]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_example, train_mask_example = train_ds[100]\n",
    "both = torch.stack([train_image_example, train_mask_example], dim=0)\n",
    "x, = torch.split(both, 2)\n",
    "# x.shape, y.shape\n",
    "y, z = x\n",
    "y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([397, 768]), torch.Size([397, 768]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bimap Legend\n",
    "\n",
    "<div style=\"font-size: 20px;\">\n",
    "After subtracting 1 from pixel value.\n",
    "\n",
    "* 1 = Bone structure (segmentation target)\n",
    "* 0 = Background pixel\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class BimapClasses(IntEnum):\n",
    "    BONE = 1\n",
    "    BACKGROUND = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=768x397>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot check a segmentation mask image after post-processing it\n",
    "t2img(train_mask_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple torchvision compatible transform to send an input tensor\n",
    "# to a pre-specified device.\n",
    "class ToDevice(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Sends the input object to the device specified in the\n",
    "    object's constructor by calling .to(device) on the object.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, img):\n",
    "        return img.to(self.device)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(device={self.device})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset wrapper that allows us to perform custom image augmentations\n",
    "# on both the target and label (segmentation mask) images.\n",
    "#\n",
    "# These custom image augmentations are needed since we want to perform\n",
    "# transforms such as:\n",
    "# 1. Random horizontal flip\n",
    "# 2. Image resize\n",
    "#\n",
    "# and these operations need to be applied consistently to both the input\n",
    "# image as well as the segmentation mask.\n",
    "class MRIDatasetAugmented(MRIDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Path,\n",
    "        split_mode: str,\n",
    "        target_types=\"segmentation\",\n",
    "        download=False,\n",
    "        pre_transform=None,\n",
    "        post_transform=None,\n",
    "        pre_target_transform=None,\n",
    "        post_target_transform=None,\n",
    "        common_transform=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root,\n",
    "            split_mode=split_mode,\n",
    "            target_types=target_types,\n",
    "            # download=download,\n",
    "            transform=pre_transform,\n",
    "            target_transform=pre_target_transform,\n",
    "        )\n",
    "        self.post_transform = post_transform\n",
    "        self.post_target_transform = post_target_transform\n",
    "        self.common_transform = common_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (input, target) = super().__getitem__(idx)\n",
    "        \n",
    "        # Common transforms are performed on both the input and the labels\n",
    "        # by creating a 4 channel image and running the transform on both.\n",
    "        # Then the segmentation mask (4th channel) is separated out.\n",
    "        if self.common_transform is not None:\n",
    "            both = torch.stack([input, target], dim=0)\n",
    "            both = self.common_transform(both)\n",
    "            (input, target) = torch.split(both, 2, dim=0)\n",
    "        # end if\n",
    "        \n",
    "        if self.post_transform is not None:\n",
    "            input = self.post_transform(input)\n",
    "        if self.post_target_transform is not None:\n",
    "            target = self.post_target_transform(target)\n",
    "\n",
    "        return (input, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
